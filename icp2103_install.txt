
-------------------- Red Hat v7.5 - ICP v2.1.0.3 Install -----------------------------------------

hostnamectl set-hostname icpboot
hostnamectl set-hostname icpproxy
hostnamectl set-hostname icpwork1
hostnamectl set-hostname icpwork2



-- ALL NODES --

1. vi /etc/hosts

#::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.35.31	icpboot
192.168.35.47	icpproxy
192.168.35.189	icpwork1
192.168.35.252	icpwork2


2. Firewall Inactive

systemctl stop firewalld
systemctl disable firewalld
systemctl status firewalld



-- MASTER NODES --

1. ensure that the vm.max_map_count setting is at least 262144. 
echo "vm.max_map_count=262144" | sudo tee -a /etc/sysctl.conf


2. ensure that the lower limit of the ephemeral port range is greater than 10240. 
echo 'net.ipv4.ip_local_port_range="10240 60999"' | sudo tee -a /etc/sysctl.conf


-- BOOT NODE --

cd /ICP_IMAGES/
chmod +x icp-docker-17.12.1_x86_64.bin
sudo ./icp-docker-17.12.1_x86_64.bin --install


/////////////////////////

2018-06-01 11:22:01 Installing docker dependent package
Loaded plugins: langpacks, product-id, search-disabled-repos, subscription-
              : manager
This system is not registered with an entitlement server. You can use subscription-manager to register.
There are no enabled repos.
 Run "yum repolist all" to see the repos you have.
 To enable Red Hat Subscription Management repositories:
     subscription-manager repos --enable <repo>
 To enable custom repositories:
     yum-config-manager --enable <repo>



subscription-manager repos --disable=*

////////////////////////////

docker 설치 안될 때
Red Hat 7.5 DVD를 넣는다. (ALL NODES)에 적용

[root@icpboot ~]# df -m
Filesystem                   1M-blocks  Used Available Use% Mounted on
/dev/sr0                          4404  4404         0 100% /run/media/root/RHEL-7.5 Server.x86_64


# mkdir /media/Redhat-disc

# mount -o ro -t iso9660 /dev/sr0 /media/Redhat-disc

# rpm --import /media/Redhat-disc/RPM-GPG-KEY-redhat-release

# vi /etc/yum.repos.d/Redhat-disc.repo

[Redhat-disc]
name=Redhat-disc
baseurl=file:///media/Redhat-disc
enabled=1
gpgcheck=1

# yum repolist

[root@icpboot Redhat-disc]# yum repolist
Loaded plugins: langpacks, product-id, search-disabled-repos, subscription-manager
This system is not registered with an entitlement server. You can use subscription-manager to register.
Redhat-disc                                                                                                                                                                                 | 4.3 kB  00:00:00     
(1/2): Redhat-disc/group_gz                                                                                                                                                                 | 145 kB  00:00:00     
(2/2): Redhat-disc/primary_db                                                                                                                                                               | 4.1 MB  00:00:00     
repo id                                                                                               repo name                                                                                              status
Redhat-disc                                                                                           Redhat-disc                                                                                            5,099
repolist: 5,099
[root@icpboot Redhat-disc]# 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

** docker install

[root@icpboot Redhat-disc]# cd /ICP_IMAGES/
[root@icpboot ICP_IMAGES]# chmod +x icp-docker-17.12.1_x86_64.bin
[root@icpboot ICP_IMAGES]# sudo ./icp-docker-17.12.1_x86_64.bin --install

Extracting docker.tar.gz
Installing docker dependent package
Installing docker
[root@icpboot ICP_IMAGES]# 
[root@icpboot ICP_IMAGES]# docker version
Client:
 Version:	17.12.1-ce
 API version:	1.35
 Go version:	go1.9.4
 Git commit:	7390fc6
 Built:	Tue Feb 27 22:15:20 2018
 OS/Arch:	linux/amd64

Server:
 Engine:
  Version:	17.12.1-ce
  API version:	1.35 (minimum version 1.12)
  Go version:	go1.9.4
  Git commit:	7390fc6
  Built:	Tue Feb 27 22:17:54 2018
  OS/Arch:	linux/amd64
  Experimental:	false
[root@icpboot ICP_IMAGES]# 



-- Boot Node --

-> Extract the images and load them into Docker. Extracting the images might take a few minutes.

# tar xf ibm-cloud-private-x86_64-2.1.0.3.tar.gz -O | sudo docker load




-> Create an installation directory to store the IBM Cloud Private configuration files in and change to that directory. 

mkdir /opt/ibm-cloud-private-2.1.0.3;  \
cd /opt/ibm-cloud-private-2.1.0.3



-> Extract the configuration files from the installer image.

sudo docker run -v $(pwd):/data -e LICENSE=accept \
ibmcom/icp-inception:2.1.0.3-ee \
cp -r cluster /data





************************************
Sharing SSH keys among cluster nodes
************************************



ssh-keygen -b 4096 -f ~/.ssh/id_rsa -N ""

cat ~/.ssh/id_rsa.pub | sudo tee -a ~/.ssh/authorized_keys

ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.35.31
ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.35.47
ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.35.189
ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.35.252

sudo systemctl restart sshd





-> Add the IP address of each node in the cluster to the /<installation_directory>/cluster/hosts file. See Setting the node roles in the hosts file.

[root@icpboot cluster]# cat hosts
[master]
192.168.35.31

[worker]
192.168.35.189
192.168.35.252


[proxy]
192.168.35.47

#[management]
#4.4.4.4

#[va]
#5.5.5.5
[root@icpboot cluster]# 


-> replace the ssh_key file with the private key file that is used to communicate with the other cluster nodes.

cd /opt/ibm-cloud-private-2.1.0.3
sudo cp ~/.ssh/id_rsa ./cluster/ssh_key


-> Move the image files for your cluster to the /<installation_directory>/cluster/images folder.

cd /opt/ibm-cloud-private-2.1.0.3
mkdir -p cluster/images; \
sudo mv /ICP_IMAGES/ibm-cloud-private-x86_64-2.1.0.3.tar.gz  cluster/images/



--> You can also set a variety of optional cluster customizations that are available in the /<installation_directory>/cluster/config.yaml file. See General settings.
sudo docker run --net=host -t -e LICENSE=accept \
-v "$(pwd)":/installer/cluster ibmcom/icp-inception:2.1.0.3-ee install


## Install in firewall enabled mode
firewall_enabled: false




**********************
Deploy the environment
**********************

cd /opt/ibm-cloud-private-2.1.0.3/cluster

sudo docker run --net=host -t -e LICENSE=accept \
-v "$(pwd)":/installer/cluster ibmcom/icp-inception:2.1.0.3-ee install


*********
Uninstall
*********


docker run -e LICENSE=accept --net=host \
-t -v "$(pwd)":/installer/cluster ibmcom/icp-inception:2.1.0.3-ee uninstall


service docker restart




POST DEPLOY MESSAGE ******************************************************************************************

The Dashboard URL: https://192.168.35.31:8443, default username/password is admin/admin

Playbook run took 0 days, 1 hours, 29 minutes, 42 seconds



*******************************************************************
Accessing your IBM® Cloud Private cluster by using the kubectl CLI
*******************************************************************

[root@icpboot cluster]# docker run -e LICENSE=accept --net=host -v /usr/local/bin:/data ibmcom/icp-inception:2.1.0.3-ee cp /usr/local/bin/kubectl /data
[root@icpboot cluster]# 



https://192.168.35.31:8443/


--> Configure client

kubectl config set-cluster cluster.local --server=https://192.168.35.31:8001 --insecure-skip-tls-verify=true
kubectl config set-context cluster.local-context --cluster=cluster.local
kubectl config set-credentials admin --token=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdF9oYXNoIjoiODE2bHp6dGlldXZndXN3MzRmOXgiLCJyZWFsbU5hbWUiOiJjdXN0b21SZWFsbSIsInVuaXF1ZVNlY3VyaXR5TmFtZSI6ImFkbWluIiwiaXNzIjoiaHR0cHM6Ly9teWNsdXN0ZXIuaWNwOjk0NDMvb2lkYy9lbmRwb2ludC9PUCIsImF1ZCI6IjlhZDI0YjQ4NmE1ZjRkOTk0OWNmZjc4ZGU3ODJmMTI1IiwiZXhwIjoxNTI3OTc1NTg3LCJpYXQiOjE1Mjc5NDY3ODcsInN1YiI6ImFkbWluIiwidGVhbVJvbGVNYXBwaW5ncyI6W119.uUs5ZdIID0aOBJimriHgVyE-l5XIEWTgtSndT5dyw0w8d_uLQ79hY1jOG0euqq8MnhTYvuwIW4a2wYvKHjxXnXNNFISaN6xu-zomSOhU6ZYJ-qdwibfxEAIBAzCE4LIWMAB9FuY9WnD7eugYCGBptQ39ianZ8X5X7VjYNn9oDAMzSMXlL4wpaRAwJUq3PmProytYo4CTPcIdXHhafXq_GzYOiMihpWm4fb96uerDqDigRKQF7JXKi3o-inCpY4eutBUqN7yZqGgAfFb37ZWGhI1C4lfEZxSLiiYpS4Pw1BtfH9BgSVkJmdK48m6C8eo_zaPm55otj4OAWN3q6971mw
kubectl config set-context cluster.local-context --user=admin --namespace=default
kubectl config use-context cluster.local-context



[root@icpboot cluster]# kubectl version
Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.0", GitCommit:"fc32d2f3698e36b93322a3465f63a14e9f0eaead", GitTreeState:"clean", BuildDate:"2018-03-26T16:55:54Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.0+icp-ee", GitCommit:"9cb64de4ca4d039c35f4a29721aa5cf787648a15", GitTreeState:"clean", BuildDate:"2018-04-27T06:32:18Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
[root@icpboot cluster]# 




///////////////// NFS Server Setting //////////////////

192.168.35.47	icpproxy (NFS setting)


[root@icpproxy ~]# systemctl start nfs-server
[root@icpproxy ~]# systemctl enable nfs-server
Created symlink from /etc/systemd/system/multi-user.target.wants/nfs-server.service to /usr/lib/systemd/system/nfs-server.service.
[root@icpproxy ~]# systemctl status nfs-server
● nfs-server.service - NFS server and services
   Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled)
   Active: active (exited) since 토 2018-06-02 22:46:16 KST; 13s ago
 Main PID: 19709 (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/nfs-server.service

 6월 02 22:46:16 icpproxy systemd[1]: Starting NFS server and services...
 6월 02 22:46:16 icpproxy systemd[1]: Started NFS server and services.
[root@icpproxy ~]#




--> mount point setting

[root@icpproxy ~]# mkdir /storage
[root@icpproxy ~]# chmod 777 /storage
[root@icpproxy ~]# echo "/storage   *(rw,no_subtree_check,async,insecure,no_root_squash)" >> /etc/exports
[root@icpproxy ~]# exportfs -a
[root@icpproxy ~]# showmount -e
Export list for icpproxy:
/storage *
[root@icpproxy ~]#

[root@icpproxy ~]# service nfs-server restart
Redirecting to /bin/systemctl restart nfs-server.service
[root@icpproxy ~]#


///////////////// NFS Client Setting //////////////////

[root@icpwork1 ~]# showmount -e icpproxy
Export list for icpproxy:
/storage *
[root@icpwork1 ~]#


[root@icpwork2 ~]# showmount -e icpproxy
Export list for icpproxy:
/storage *
[root@icpwork2 ~]#


--> work1

[root@icpwork1 ~]# mkdir -p /nfs/storage
[root@icpwork1 ~]# chmod -R 777 /nfs
[root@icpwork1 ~]# mount -t nfs -o sync icpproxy:/storage /nfs/storage
[root@icpwork1 ~]# df -m
Filesystem                   1M-blocks  Used Available Use% Mounted on
icpproxy:/storage               236429 32568    203861  14% /nfs/storage



vim /etc/fstab

icpproxy:/storage         /nfs/storage            nfs     sync            0 0



--> work2

[root@icpwork2 ~]# mkdir -p /nfs/storage
[root@icpwork2 ~]# chmod -R 777 /nfs
[root@icpwork2 ~]# mount -t nfs -o sync icpproxy:/storage /nfs/storage
[root@icpwork2 ~]# df -m
Filesystem                   1M-blocks  Used Available Use% Mounted on
icpproxy:/storage               236429 32568    203861  14% /nfs/storage


vim /etc/fstab

icpproxy:/storage         /nfs/storage            nfs     sync            0 0


--------------------------------------------------------------------


//// ICP Storage Setting /////

Platform -> Storage -> Create PersistentVolume


* General
 - Name : nfsstorage01
 - Capacity : 50
 - Storage type : NFS

* Parameters
 - server, icpproxy
 - path, /storage






------------- Catalog -> sample deploy

Manage -> Helm Repositories -> Sync repositories -> OK

Catalog


---------------------------------------


[root@icpboot cluster]# docker login mycluster.icp:8500



[root@icpboot ~]# docker images | grep websphere
[root@icpboot ~]# 





[root@icpwork2 ~]# docker images | grep websphere
websphere-liberty                         latest              5289809d4a6c        20 hours ago        510MB
[root@icpwork2 ~]#



[root@icpwork2 ~]# docker push mycluster.icp:8500/default/websphere-liberty:latest
The push refers to repository [mycluster.icp:8500/default/websphere-liberty]
An image does not exist locally with the tag: mycluster.icp:8500/default/websphere-liberty


[root@icpwork2 ~]# docker tag websphere-liberty:latest mycluster.icp:8500/default/websphere-liberty:0.1
[root@icpwork2 ~]#
[root@icpwork2 ~]# docker images | grep websphere
websphere-liberty                              latest              5289809d4a6c        21 hours ago        510MB
mycluster.icp:8500/default/websphere-liberty   0.1                 5289809d4a6c        21 hours ago        510MB
[root@icpwork2 ~]# set -o vi
[root@icpwork2 ~]#
[root@icpwork2 ~]# docker push mycluster.icp:8500/default/websphere-liberty:0.1
The push refers to repository [mycluster.icp:8500/default/websphere-liberty]
8383000f3d47: Pushed
297696024008: Pushed
662564a0b41e: Pushed
8e395cbd3ace: Pushed
08cb38d0677d: Pushed
0b9a2af7beac: Pushed
56bfe07d87f8: Pushed
11ca6a06a732: Pushed
f36404ebda48: Pushed
9b9775ad094a: Pushed
bf3d982208f5: Pushed
cd7b4cc1c2dd: Pushed
3a0404adc8bd: Pushed
82718dbf791d: Pushed
c8aa3ff3c3d3: Pushed
0.1: digest: sha256:913f27d237b3a0551aa6406814568dea829eb6407692ac21b3d72b5786e2dbfa size: 3456
[root@icpwork2 ~]#


[root@icpboot ~]# kubectl get images
NAME                AGE
websphere-liberty   1m
[root@icpboot ~]# 


------------------------------

ICP Console -> Manage -> Images


Name				Owner	Scope		Action
default/websphere-liberty	default	namespace	


